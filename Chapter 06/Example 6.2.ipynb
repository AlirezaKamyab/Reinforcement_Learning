{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500c0513-e247-4fad-8343-bdcde088fdf3",
   "metadata": {},
   "source": [
    "# Example 6.2 Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c203e213-0303-44fd-8cb3-28351384b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e279f8-5184-45ec-87c8-1d6a40a0fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk:\n",
    "    def __init__(self):\n",
    "        self.current_state = None\n",
    "\n",
    "    def reset(self):\n",
    "        # State starts at C\n",
    "        ## States are [T1, A, B, C, D, E, T2]\n",
    "        self.current_state = 3\n",
    "        return {\"state\":self.current_state}\n",
    "\n",
    "    def is_terminated(self):\n",
    "        if self.current_state in [0, 6]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self, action:int):\n",
    "        # 0 means left and 1 means right\n",
    "        if self.current_state is None:\n",
    "            raise ValueError(\"Please reset the environment first\")\n",
    "        if self.is_terminated():\n",
    "            return {\"state\": self.current_state, \"action\":action, \"reward\": 0.0}\n",
    "        if action not in [0, 1]:\n",
    "            raise ValueError(\"Invalid action taken\")\n",
    "        \n",
    "        if action == 0:\n",
    "            self.current_state -= 1\n",
    "        else:\n",
    "            self.current_state += 1\n",
    "\n",
    "        reward = 0.0\n",
    "        if self.current_state == 6:\n",
    "            reward = +1.0\n",
    "\n",
    "        return {\"state\":self.current_state, \"action\":action, \"reward\":reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420ef4c6-726f-4269-99d1-973fb9046ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(\n",
    "    V_init:np.ndarray,\n",
    "    V_star:np.ndarray,\n",
    "    policy:np.ndarray,\n",
    "    episodes:int=100,\n",
    "    gamma:float=1.0,\n",
    "    alpha:float=0.1, \n",
    "    runs:int=1\n",
    "):\n",
    "    random_walk = RandomWalk()\n",
    "    history = [{'RMS':0, 'V':V_init.copy()} for _ in range(episodes)]\n",
    "\n",
    "    for r in range(runs):\n",
    "        V = V_init.copy()\n",
    "        for episode in range(episodes):\n",
    "            random_walk.reset()\n",
    "    \n",
    "            while not random_walk.is_terminated():\n",
    "                # Take the current state\n",
    "                current_state = random_walk.current_state\n",
    "                # The action is taken according to the policy\n",
    "                action = np.random.choice([0, 1], p=policy[random_walk.current_state])\n",
    "                # Take the step to see what reward will be observed, and what is the new state\n",
    "                feedback = random_walk.step(action)\n",
    "                new_state, reward = feedback['state'], feedback['reward']\n",
    "                # Update the value function: V(s) = V(s) + \\alpha * (r + \\gamma * V(s') - V(s))\n",
    "                V[current_state] = V[current_state] + alpha * (reward + gamma * V[new_state] - V[current_state])\n",
    "\n",
    "            RMS = np.sqrt(np.mean(np.square(V[1:-1] - V_star[1:-1])))\n",
    "            history[episode]['RMS'] += 1 / (r + 1) * (RMS - history[episode]['RMS'])\n",
    "            history[episode]['V'] += 1 / (r + 1) * (V - history[episode]['V'])\n",
    "\n",
    "    history.insert(0, \n",
    "                   {'RMS':np.sqrt(np.mean(np.square(V_init[1:-1] - V_star[1:-1]))), \n",
    "                    'V':V_init.copy()})\n",
    "    return V, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c99ffe-e8b5-4fda-95ed-3e812a3909b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_init = np.array([0, 1/2, 1/2, 1/2, 1/2, 1/2, 0], dtype=np.float32)\n",
    "V_star = np.array([0, 1/6, 2/6, 3/6, 4/6, 5/6, 0], dtype=np.float32)\n",
    "policy = np.ones(shape=(7, 2), dtype=np.float32) * 0.5\n",
    "histories = []\n",
    "alphas = [0.01, 0.02, 0.03, 0.15, 0.04, 0.05, 0.1]\n",
    "\n",
    "for alpha in alphas:\n",
    "    V, history = TD_evaluation(V_init=V_init, V_star=V_star, policy=policy, episodes=100, gamma=1.0, alpha=alpha, runs=100)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c875b25-7856-4885-9d8c-68ad64979fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "markers = ['x--', '^-', '-', '.-.']\n",
    "\n",
    "for i, walk in enumerate([0, 1, 10, 100]):\n",
    "    ax.plot(range(5), histories[-1][walk]['V'][1:-1], markers[i], label=walk);\n",
    "ax.plot(range(5), V_star[1:-1], '--', label='True values')\n",
    "ax.grid(c='#eee')\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(['A', 'B', 'C', 'D', 'E'])\n",
    "ax.set_title('Estimated value')\n",
    "ax.set_xlabel('State')\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    ax.plot(range(len(histories[i])), np.array([x['RMS'] for x in histories[i]]), label=rf'$\\alpha={alpha}$');\n",
    "\n",
    "ax.set_title('Epirical RMS error, averaged over states')\n",
    "ax.set_xlabel('Walks/Episodes')\n",
    "ax.set_ylabel('RMS')\n",
    "ax.grid(c='#eee')\n",
    "\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
