{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c456e061-95e7-444f-ba8f-0bf964c6f9d8",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation\n",
    "\n",
    "From previous chapter we've studied Bellman Optimality Equations<br>\n",
    "For state value function:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_*(s) &= \\underset{a}{max}\\ \\mathbb{E}\\big[R_{t+1} + \\gamma v_*(S_{t + 1}) | S_t = s, A_t=a\\big]\\\\\n",
    "    &= \\underset{a}{max}\\sum_{s^{\\prime}, r}p(s^{\\prime}, r | s, a)\\big[r + \\gamma v_*(S_{t+1})\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For state-action pair value function:\n",
    "$$\n",
    "\\begin{align}\n",
    "    q_*(s, a) &= \\mathbb{E}\\big[R_{t+1} + \\gamma\\ \\underset{a^{\\prime}}{max}\\ q_*(S_{t+1}, a^{\\prime}) | S_t=s, A_t=a\\big]\\\\\n",
    "    &= \\sum_{s^{\\prime}, r}p(s^{\\prime}, r|s, a)\\ \\underset{a^{\\prime}}{max}\\big[r + \\gamma q_*(s^{\\prime}, a^{\\prime})\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Policy Evaluation\n",
    "Also from previous chapter we've studied how we can compute $v_\\pi(s)$ via $v_\\pi(s^{\\prime})$ given that $s^\\prime$ is the successor state.\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_\\pi(s) &= \\mathbb{E}\\big[G_t | S_t=s\\big]\\\\\n",
    "    &= \\mathbb{E}\\big[R_{t+1} + \\gamma G_{t+1} | S_t=s\\big]\\\\\n",
    "    &= \\mathbb{E}\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s\\big]\\\\\n",
    "    &= \\sum_{a}\\pi(a|s)\\sum_{s^\\prime, r}p(s^\\prime, r | s, a)\\big[r + \\gamma v_\\pi(s^\\prime)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $v_\\pi(s)$ shows how good it is to be in state $s$ given that we follow policy $\\pi$\n",
    "- $\\pi(a|s)$ shows the probability that action $a$ will be taken given that we are in state $s$\n",
    "\n",
    "### iterative method\n",
    "We are to use an update rule to approximate $v_\\pi$ starting from $v_0$ which is an arbitrary valued value-function to get to the approximated value-function. $(v_0, v_1, v_2, \\dots, v_\\pi)$<br>\n",
    "Update rule is:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{k+1}(s) &= \\mathbb{E}_\\pi\\big[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t=s\\big]\\\\\n",
    "    &= \\sum_{a}\\pi(a|s)\\sum_{s^\\prime, r}p(s^\\prime, r | s, a)\\big[r + \\gamma v_k(s^\\prime)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Algorithm is guaranteed to converge for $k\\rightarrow\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82318ddd-3bdc-4929-9b03-9b52fb02dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad92f8a8-94ac-4720-b4b0-cf041baf0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid_world_shape, rewards, transition, policy, gamma=1.0, threshold=1e-12):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        grid_world_shape: shape of the grid world in 2D\n",
    "        rewards:          denotes rewards function given state and action and the new-state, SxA\n",
    "        transition:       function denoting transition probability from state s to s_prime, SxAxS\n",
    "        policy:           the policy (pi) to be evaluated\n",
    "        gamma:            discounting factor\n",
    "        threshold:        determines the accuracy of estimation\n",
    "    returns:\n",
    "        v_pi:             approximation of policy evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Define V(s) arbitrary and for terminal states, V(terminal) = 0\n",
    "    V = np.zeros(grid_world_shape)\n",
    "\n",
    "    # Take the number of rows and columns in grid world shape\n",
    "    rows, columns = grid_world_shape\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        # i and j together denote state s\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                v = V[i, j]\n",
    "                # for each action find its expected return given action a is taken\n",
    "                new_vs = 0\n",
    "                for a in range(4):\n",
    "                    # i_prime and j_prime denote state s_prime\n",
    "                    for i_prime in range(rows):\n",
    "                        for j_prime in range(columns):\n",
    "                            new_vs += pi[a, i, j] * transition[i, j, a, i_prime, j_prime] * (reward[i, j, a] + gamma * V[i_prime, j_prime])\n",
    "                # Update the value for state s\n",
    "                V[i, j] = new_vs \n",
    "\n",
    "                # Storing the difference that each update makes\n",
    "                delta = max(delta, np.abs(v - V[i, j]))\n",
    "\n",
    "        # If the maximum difference made is less than the threshold break\n",
    "        if delta <= threshold: break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff367a6-ab2e-4da9-aaf3-a43fe9c22221",
   "metadata": {},
   "source": [
    "### Definition of the MDP environment\n",
    "Below we've defined the Markov-Decision-Process's Environment, reward signal and dynamics.<br>\n",
    "Also we've defined a policy $\\pi$ in which actions are chosen from a uniform probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed5f1fd-06af-444c-91fe-961922f73045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the shape of the grid world\n",
    "grid_world_shape = (4, 4)\n",
    "\n",
    "\n",
    "# Defining the reward function, SxAxS\n",
    "reward = np.zeros(grid_world_shape + (4,)) - 1\n",
    "reward[0, 0, :] = 0                                 # Terminal state at top left corner\n",
    "reward[-1, -1, :] = 0                               # Terminal state at bottom right corner\n",
    "\n",
    "\n",
    "# Defining the transition function\n",
    "transition = np.zeros(grid_world_shape + (4,) + grid_world_shape)\n",
    "# Let's define the transition function\n",
    "for a in range(4):                                  # let's denote 0: up, 1:right, 2:down, 3:left\n",
    "    for i in range(grid_world_shape[0]):\n",
    "        for j in range(grid_world_shape[1]):\n",
    "            if a == 0:                    \n",
    "                transition[i, j, 0, max(0, i - 1), j] = 1\n",
    "            if a == 1 :\n",
    "                transition[i, j, 1, i, min(grid_world_shape[1] - 1, j + 1)] = 1\n",
    "            if a == 2:\n",
    "                transition[i, j, 2, min(grid_world_shape[0] - 1, i + 1), j] = 1\n",
    "            if a == 3:\n",
    "                transition[i, j, 3, i, max(j - 1, 0)] = 1\n",
    "# Change the transition function for terminal states\n",
    "transition[0, 0, :, :, :] = 0\n",
    "transition[0, 0, :, 0, 0] = 1\n",
    "transition[-1, -1, :, :, :] = 0\n",
    "transition[-1, -1, :, -1, -1] = 1\n",
    "\n",
    "\n",
    "# Define the policy in which actions are chosen from a uniform probability distribution\n",
    "pi = np.ones((4,) + grid_world_shape) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5538c3-2935-48de-8300-094c0e36b395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_policy_evaluation(grid_world_shape, reward, transition, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398166ee-7af7-4523-bf7c-76a39db26f88",
   "metadata": {},
   "source": [
    "- Note that here we've used ```transition``` instead of dynamics of the environment for better implementation since rewards are not stochastic and are not chosen from a probability distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
