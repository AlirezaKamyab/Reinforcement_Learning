{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0922eae-8553-4750-9b45-dceeac74b04f",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "## Policy Improvement\n",
    "\n",
    "Knowing the value function $v_\\pi$, we want to know whether there is a deterministic policy that we can choose and is better than current policy. One way is to choose an action $a$ and follow policy $\\pi$ thereafter. The value of behaving this way is:\n",
    "$$\n",
    "\\begin{align}\n",
    "    q_\\pi(s, a) &= \\mathbb{E}\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s, A_t=a\\big]\\\\\n",
    "    &= \\sum_{s^\\prime, r}p(s^\\prime, r | s, a)\\big[r + \\gamma v_\\pi(s^\\prime)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "Now if, $q_\\pi(s, a)$ is actually better than $v_\\pi$ then it would be better to take action $a$ everytime $s$ is encountered rather than following policy $\\pi$ all the time; infact it is always better to take action $a$ if the state is $s$.\n",
    "\n",
    "### Policy Improvement theorem\n",
    "Let $\\pi$ and $\\pi^\\prime$ be any pair of deterministic policies such that, for all $s \\in \\mathcal{S}$:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    q_\\pi(s, \\pi^\\prime(s)) \\ge v_\\pi(s)\n",
    "\\end{equation}\n",
    "$$\n",
    "then policy $\\pi^\\prime$ must be as good as or better than policy $\\pi$, given that policy $\\pi$ shows that by following policy $\\pi$ at state $s$ we get $v_\\pi(s)$ which is less than or equal to what we get if we take action $\\pi^\\prime(s)$ at state $s$ and following policy $\\pi$ thereafter.\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{\\pi^\\prime}(s) \\ge v_\\pi(s)\n",
    "\\end{align}\n",
    "$$\n",
    "So, begin at state $s$ under policy $\\pi^\\prime$ is better.<br>\n",
    "Two policies $\\pi^\\prime$ and $\\pi$ are identical except for state $s$ that is $\\pi^\\prime(s)=a\\neq \\pi(s)$. Also if there is strict inequality $q_\\pi(s, a) \\gt v_\\pi(s)$ then the changed policy is better than policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f7076-c11c-4d1e-9d5c-5cfc420f6007",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_\\pi(s) &\\le q_\\pi(s, \\pi^\\prime(s))\\\\\n",
    "    &= \\mathbb{E}\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s, A_t=\\pi^\\prime(s)\\big]\\\\\n",
    "    &= \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s\\big]\\\\\n",
    "    &\\le \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi^\\prime(S_{t+1})) | S_t=s\\big]\\\\\n",
    "    &= \\mathbb{E}_{\\pi^\\prime}\\bigg[R_{t+1} + \\gamma \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+2} + \\gamma v_\\pi(S_{t+2}) | S_{t+1}, A_{t+1}=\\pi^\\prime(S_{t+1})\\big] | S_t=s\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2}) | S_t=s\\big]\\\\\n",
    "    &= \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 v_\\pi(S_{t+3}) | S_t=s\\big]\\\\\n",
    "    &\\vdots\\\\\n",
    "    &= \\mathbb{E}_{\\pi^\\prime}\\big[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots) | S_t=s\\big]\\\\\n",
    "    &= v_{\\pi^\\prime}(s)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd107eb2-2a2b-4648-98c7-d24a49fa28e7",
   "metadata": {},
   "source": [
    "Given explaination above it is prominent that we should consider changes at all states and to all possible actions; given this, we should choose each action in each states greedily according to $q_\\pi(s, a)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pi^\\prime(s) &= \\underset{a}{argmax}\\ q_\\pi(s, a)\\\\\n",
    "    &= \\underset{a}{argmax}\\ \\mathbb{E}\\big[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s, A_t=a\\big]\\\\\n",
    "    &= \\underset{a}{argmax}\\ \\sum_{s^\\prime, r}p(s^\\prime, r | s, a)\\big[r + \\gamma v_\\pi(s^\\prime)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "although greedy policy only chooses the best short term action in one step look ahead but since it meets the conditions for policy improvement theorem, we should know that it is as good as or better than the original policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5542af-1b65-4544-a1b7-592435d288d3",
   "metadata": {},
   "source": [
    "#### Policy improvement converges to $\\pi_*$\n",
    "We have shown that by improving policy $\\pi$ to get policy $\\pi^\\prime$, the new policy is always either as good as original policy or better than the original policy.\n",
    "Let's say a policy is as good as but not better than the old policy $\\pi$. Then $v_\\pi = v_{\\pi^\\prime}$ for all of the $s \\in \\mathcal{S}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{\\pi^\\prime}(s) &= \\underset{a}{max}\\ \\mathbb{E}\\big[R_{t+1} + \\gamma v_{\\pi^\\prime}(S_{t+1}) | S_t=s, A_t=a\\big]\\\\\n",
    "    &= \\underset{a}{max}\\ \\sum_{s^\\prime, r}p(s^\\prime, r | s, a)\\big[r + \\gamma v_{\\pi^\\prime}(s^\\prime)\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "And this is the same as Bellman Optimality Equation.<br>\n",
    "Thus, $v_{\\pi^\\prime}$ is $v_*$ and $\\pi$ and $\\pi^\\prime$ are optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a93ae4-0c9b-4598-a5b3-76337b8dac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c421d23e-edef-4f93-a54e-5caca91a8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid_world_shape, rewards, transition, policy, V, gamma=1.0, \n",
    "                                threshold=1e-12):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        grid_world_shape: shape of the grid world in 2D\n",
    "        rewards:          denotes rewards function given state and action and the new-state, SxA\n",
    "        transition:       function denoting transition probability from state s to s_prime, SxAxS\n",
    "        policy:           the policy (pi) to be evaluated\n",
    "        V:                estimated value at each state\n",
    "        gamma:            discounting factor\n",
    "        threshold:        determines the accuracy of estimation\n",
    "    returns:\n",
    "        v_pi:             approximation of policy evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Take the number of rows and columns in grid world shape\n",
    "    rows, columns = grid_world_shape\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        # i and j together denote state s\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                v = V[i, j]\n",
    "                # for each action find its expected return given action a is taken\n",
    "                new_vs = 0\n",
    "                for a in range(4):\n",
    "                    # i_prime and j_prime denote state s_prime\n",
    "                    for i_prime in range(rows):\n",
    "                        for j_prime in range(columns):\n",
    "                            new_vs += policy[a, i, j] * transition[i, j, a, i_prime, j_prime] * (reward[i, j, a] + gamma * V[i_prime, j_prime])\n",
    "                # Update the value for state s\n",
    "                V[i, j] = new_vs \n",
    "\n",
    "                # Storing the difference that each update makes\n",
    "                delta = max(delta, np.abs(v - V[i, j]))\n",
    "\n",
    "        # If the maximum difference made is less than the threshold break\n",
    "        if delta <= threshold: break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a4faf9-3886-439e-a5ae-fdbfed43a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(grid_world_shape, rewards, transition, policy, value_function, gamma=1.0):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        grid_world_shape: shape of the grid world in 2D\n",
    "        rewards:          denotes rewards function given state and action and the new-state, SxA\n",
    "        transition:       function denoting transition probability from state s to s_prime, SxAxS\n",
    "        policy:           the policy (pi) to be evaluated\n",
    "        value_function:   estimated value at each state\n",
    "        gamma:            discounting factor\n",
    "    returns:\n",
    "        policy:           improved policy\n",
    "    \"\"\"\n",
    "    # Take the number of rows and columns in grid world shape\n",
    "    rows, columns = grid_world_shape\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            # actions: stores expected value each action can observe given the state and action pair, q_pi\n",
    "            actions = np.zeros((4,))\n",
    "            for i_prime in range(rows):\n",
    "                for j_prime in range(columns):\n",
    "                    # calculates q_pi for each action\n",
    "                    actions += transition[i, j, :, i_prime, j_prime] * (rewards[i, j] + gamma * value_function[i_prime, j_prime])\n",
    "            policy[:, i, j] = np.zeros((4,))\n",
    "            # Improves the policy\n",
    "            policy[np.argmax(actions), i, j] = 1\n",
    "                \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251363e5-3689-4e2d-bbd2-d69f5a474f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(grid_world_shape, rewards, transition, gamma=1.0, threshold=1e-12):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        grid_world_shape: shape of the grid world in 2D\n",
    "        rewards:          denotes rewards function given state and action and the new-state, SxA\n",
    "        transition:       function denoting transition probability from state s to s_prime, SxAxS\n",
    "        gamma:            discounting factor\n",
    "        threshold:        determines the accuracy of estimation\n",
    "    returns:\n",
    "        v_star:           approximation of optimal policy evaluation\n",
    "        pi_star:          optimal policy\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    V = np.zeros(grid_world_shape)\n",
    "    policy = np.ones((4,) + grid_world_shape) / 4\n",
    "\n",
    "    # Stores whether policy has improved\n",
    "    policy_stable = False\n",
    "    \n",
    "    while not policy_stable:\n",
    "        policy_stable = True\n",
    "        V = iterative_policy_evaluation(grid_world_shape, rewards, transition, policy, V, gamma, threshold)\n",
    "        # Stores the old policies greedy actions to see whether it has been improved\n",
    "        old_policy = np.argmax(policy, axis=0)\n",
    "        # Improvement\n",
    "        policy = policy_improvement(grid_world_shape, rewards, transition, policy, V, gamma)\n",
    "        # If policy has improved, it means that policy is not stable yet\n",
    "        if not np.array_equal(old_policy, np.argmax(policy, axis=0)):\n",
    "            policy_stable = False\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bb1455-8f20-4f28-a5d1-e8c2cc357c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the shape of the grid world\n",
    "grid_world_shape = (4, 4)\n",
    "\n",
    "\n",
    "# Defining the reward function, SxAxS\n",
    "reward = np.zeros(grid_world_shape + (4,)) - 1\n",
    "reward[0, 0, :] = 0                                 # Terminal state at top left corner\n",
    "reward[-1, -1, :] = 0                               # Terminal state at bottom right corner\n",
    "\n",
    "\n",
    "# Defining the transition function\n",
    "transition = np.zeros(grid_world_shape + (4,) + grid_world_shape)\n",
    "# Let's define the transition function\n",
    "for a in range(4):                                  # let's denote 0: up, 1:right, 2:down, 3:left\n",
    "    for i in range(grid_world_shape[0]):\n",
    "        for j in range(grid_world_shape[1]):\n",
    "            if a == 0:                    \n",
    "                transition[i, j, 0, max(0, i - 1), j] = 1\n",
    "            if a == 1 :\n",
    "                transition[i, j, 1, i, min(grid_world_shape[1] - 1, j + 1)] = 1\n",
    "            if a == 2:\n",
    "                transition[i, j, 2, min(grid_world_shape[0] - 1, i + 1), j] = 1\n",
    "            if a == 3:\n",
    "                transition[i, j, 3, i, max(j - 1, 0)] = 1\n",
    "# Change the transition function for terminal states\n",
    "transition[0, 0, :, :, :] = 0\n",
    "transition[0, 0, :, 0, 0] = 1\n",
    "transition[-1, -1, :, :, :] = 0\n",
    "transition[-1, -1, :, -1, -1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50261d81-c5d6-4714-860c-4d4010fb90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "V, policy = policy_iteration(grid_world_shape, reward, transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1bcfb2-99fd-415f-b989-48c423ab0c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9aa218-dadc-4a7a-89cf-2a394b041beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 2],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 1, 2],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(policy, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
