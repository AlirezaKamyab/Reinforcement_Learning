{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31a4c040-e98a-4ed4-9f2f-236bbc78cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787165c-b590-40da-bd1b-6f49fe02785b",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5507117-b041-440e-9de7-40b1c891fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake:\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_steps:int=16, \n",
    "        is_slippery:bool=True, \n",
    "        render:bool=True, \n",
    "        custom_reward: bool=True\n",
    "    ):\n",
    "        self.max_steps = max_steps\n",
    "        self.render = render\n",
    "        self.custom_reward = custom_reward\n",
    "        self.frozen_lake = gym.make(\n",
    "            'FrozenLake-v1',\n",
    "            desc=None,\n",
    "            map_name=\"4x4\",\n",
    "            is_slippery=is_slippery,\n",
    "            render_mode='rgb_array' if render else None\n",
    "        )\n",
    "\n",
    "    def generate_episode(\n",
    "        self, \n",
    "        policy: np.ndarray, \n",
    "        epsilon:float=0.1\n",
    "    ):\n",
    "        state, _ = self.frozen_lake.reset()\n",
    "        if not self.custom_reward:\n",
    "            reward = 0.0\n",
    "        else:\n",
    "            reward = -np.sqrt(18)\n",
    "        trajectory = []\n",
    "        # Render RGB trajectory to observe the states visually\n",
    "        if self.render:\n",
    "            render = [self.frozen_lake.render()]\n",
    "            \n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, self.frozen_lake.action_space.n)\n",
    "            else:\n",
    "                action = np.argmax(policy[state])\n",
    "            new_state, new_reward, terminated, _, _ = self.frozen_lake.step(action)\n",
    "            if self.custom_reward:\n",
    "                x = new_state / 4\n",
    "                y = new_state % 4\n",
    "                new_reward = -np.sqrt((x-3)**2 + (y-3)**2)\n",
    "\n",
    "            trajectory.append({'reward':reward, 'state':state, 'action':action})\n",
    "\n",
    "            # Render RGB trajectory to observe the states visually\n",
    "            if self.render:\n",
    "                render.append(self.frozen_lake.render())\n",
    "                \n",
    "            reward = new_reward\n",
    "            state = new_state\n",
    "\n",
    "            if self.max_steps <= len(trajectory) - 1:\n",
    "                break\n",
    "\n",
    "        trajectory.append({'reward':reward})\n",
    "        if self.render:\n",
    "            return trajectory, render\n",
    "        return trajectory\n",
    "\n",
    "    def generate_video(self, frames, output_name:str='output.mp4', fps:float=1.5):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_writer = cv2.VideoWriter(output_name, fourcc, fps, (256, 256))\n",
    "        for frame in render:\n",
    "            frame = frame[:, :, ::-1]\n",
    "            video_writer.write(frame)\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c69e1-7521-4950-bb23-97616ded531f",
   "metadata": {},
   "source": [
    "## On-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b2d0bdd-327c-4bae-b874-e8c421a53223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–                      | 20362/1000000 [01:23<1:06:44, 244.61it/s, G=-5.46]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(episodes)) \u001b[38;5;28;01mas\u001b[39;00m prog:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m prog:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         trajectory, frames = \u001b[43mfrozen_lake\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m         G = \u001b[32m0.0\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trajectory) - \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mFrozenLake.generate_episode\u001b[39m\u001b[34m(self, policy, epsilon)\u001b[39m\n\u001b[32m     43\u001b[39m     x = new_state / \u001b[32m4\u001b[39m\n\u001b[32m     44\u001b[39m     y = new_state % \u001b[32m4\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     new_reward = -\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m trajectory.append({\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m:reward, \u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m:state, \u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m:action})\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Render RGB trajectory to observe the states visually\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "num_actions = 4\n",
    "state_set = 16\n",
    "gamma = 1.0\n",
    "render_every_episodes = 1000\n",
    "episodes = 1000000\n",
    "policy = np.ones([state_set, num_actions], dtype=np.float32) * epsilon / num_actions\n",
    "policy[:, 0] = 1 - epsilon + epsilon / num_actions \n",
    "Q = np.zeros([state_set, num_actions], dtype=np.float32)\n",
    "N = np.zeros([state_set, num_actions], dtype=np.float32)\n",
    "frozen_lake = FrozenLake()\n",
    "\n",
    "with tqdm(range(episodes)) as prog:\n",
    "    for episode in prog:\n",
    "        trajectory, frames = frozen_lake.generate_episode(policy=policy, epsilon=epsilon)\n",
    "        G = 0.0\n",
    "\n",
    "        for i in reversed(range(len(trajectory) - 1)):\n",
    "            reward = trajectory[i + 1]['reward']\n",
    "            state = trajectory[i]['state']\n",
    "            action = trajectory[i]['action']\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            is_visited = any([t['state'] == state and t['action'] == action for t in trajectory[:i]])\n",
    "            if not is_visited:\n",
    "                state, action = int(state), int(action)\n",
    "                N[state, action] += 1\n",
    "                Q[state, action] = Q[state, action] + (1 / N[state, action]) * (G - Q[state, action])\n",
    "                greedy_action = np.argmax(Q[state])\n",
    "                for a in range(num_actions):\n",
    "                    if a == greedy_action:\n",
    "                        policy[state, a] = 1 - epsilon + epsilon / num_actions\n",
    "                    else:\n",
    "                        policy[state, a] = epsilon / num_actions\n",
    "\n",
    "        if episode % render_every_episodes == 0:\n",
    "            frozen_lake.generate_video(frames, fps=2)\n",
    "        prog.set_postfix({'G': np.mean(Q)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be010fda-cf3a-4f39-bc3e-74db5af78085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
